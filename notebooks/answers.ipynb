{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_formats = ['svg']\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jak tabulky interpretovat\n",
    "\n",
    "Každá potenciální odpověď má jeden řádek.\n",
    "\n",
    "### Correct/All evals ratio\n",
    "\n",
    "- by ideálně mělo být 100 %\n",
    "- indikuje to, v kolika procentech odevzání byla daná možnost správně (ne)zaškrtnutá\n",
    "    - i.e. 100 % by znamenalo, že pokud je to\n",
    "        - správná odpověď, tak byla ve všech odevzdáních zaškrtnuta\n",
    "        - špatná odpověď, tak nebyla v žádném odevzdání zaškrtnuta\n",
    "- potenciální problém: jeden řešitel, který neví a tipuje, má větší efekt na procento než řešitel, který to dá na první pokus správně\n",
    "\n",
    "**Důsledky pro seminář**:\n",
    "\n",
    "- odpovědi, které mají nízké procento (třeba méně než < 80 %), jsou \"chytáky\", nebo potenciálně špatně vysvětlené\n",
    "\n",
    "\n",
    "### Count evals\n",
    "\n",
    "- absolutní počet odevzdání, které měli danou odpověď zaškrtnutou\n",
    "- pro správné odpovědi je podkreslení zelené\n",
    "- pro špatné odpovědi je podkreslení červené\n",
    "- potenciální problém stejný jako pro předchozí sloupec: jeden řešitel, který neví a tipuje, má větší efekt na procento než řešitel, který to dá na první pokus správně\n",
    "\n",
    "\n",
    "### Ticked at least once/All users ratio\n",
    "\n",
    "- Procento řešitelů, kteří odpověď zaškrtli alespoň jednou, ze všech řešitelů, kteří modul řešili.\n",
    "- Pro špatné odpovědi bychom ideálně chtěli 0 %, pro správně odpovědi ideálně 100 %.\n",
    "- 100 % na správné odpovědi je špatně dosažitelných, stačí jeden řešitel jež úlohu odevzdal neúspěšně a pak ji nikdy úspěšně nedokončil.\n",
    "\n",
    "**Důsledky pro seminář**:\n",
    "\n",
    "Špatné odpovědi by měly mít malé procento, pokud nemají, tak jsou to chytáky, nebo špatně vysvětlené.\n",
    "\n",
    "Správné odpovědi by měli mít velké procento, pokud nemají, tak jsme nejspíš nedostatečně vysvětlili, proč je tvrzení pravdivé.\n",
    "\n",
    "Opakovaně neúspěšní/tipující řešitelé nemají takový vliv na výsledná procenta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict, namedtuple\n",
    "from sqlalchemy import func, distinct, text, and_\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import util\n",
    "from util.year import year as current_year\n",
    "from db import session\n",
    "import model\n",
    "from datetime import datetime\n",
    "import re\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "plt.rcParams['figure.figsize'] = [8, 6]\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations = session.query(\n",
    "    model.EvaluationParticipantsWithContext\n",
    ")\\\n",
    ".filter(model.EvaluationParticipantsWithContext.year_id == current_year.id)\\\n",
    ".filter(model.EvaluationParticipantsWithContext.type == \"quiz\")\\\n",
    ".order_by(model.EvaluationParticipantsWithContext.task_id, model.EvaluationParticipantsWithContext.module_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = evaluations.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer = namedtuple(\"Answer\", [\"evaluation\", \"user_answers\", \"correct_answers\"])\n",
    "Stats = namedtuple(\"Stats\", [\"evaluation\", \"correct_answer\", \"combination_counts\", \"item_counts\", \"item_users\", \"total_evaluations\", \"users\"])\n",
    "\n",
    "\n",
    "RE_ANSWER_LINE = re.compile(r\"^\\s*\\[[yn]\\] Question \\d+ -- user answers: \\[(.*)\\], correct answers: \\[(.*)\\]\")\n",
    "\n",
    "\n",
    "def answer_to_list(answer):\n",
    "    return tuple() if not answer else tuple(int(a) for a in answer.split(\", \"))\n",
    "\n",
    "def extract_answer(evaluation):\n",
    "    lines = evaluation.full_report.split(\"\\n\")\n",
    "    user_answers = []\n",
    "    correct_answers = []\n",
    "    for line in lines:\n",
    "        if line.strip().startswith(\"[\"):\n",
    "            match = RE_ANSWER_LINE.match(line)\n",
    "            assert match\n",
    "            user_answers.append(answer_to_list(match.group(1)))\n",
    "            correct_answers.append(answer_to_list(match.group(2)))\n",
    "    return Answer(evaluation, tuple(user_answers), tuple(correct_answers))\n",
    "\n",
    "def extract_stats(evaluations):\n",
    "    answers = [extract_answer(e) for e in evaluations]\n",
    "\n",
    "    result = []\n",
    "    prev_module_id = None\n",
    "    for i, evaluation in enumerate(evaluations):\n",
    "        if prev_module_id is None or prev_module_id != evaluation.module_id:\n",
    "            result.append(Stats(evaluation, answers[i].correct_answers, Counter(), Counter(), {}, Counter(), set()))\n",
    "            prev_module_id = evaluation.module_id\n",
    "        user_answers = answers[i].user_answers\n",
    "        result[-1].combination_counts.update([user_answers])\n",
    "        result[-1].item_counts.update([(item, answer_item) for item, answer in enumerate(user_answers) for answer_item in answer])\n",
    "        for item, answer in enumerate(user_answers):\n",
    "            for answer_item in answer:\n",
    "                result[-1].item_users[(item, answer_item)] = result[-1].item_users.get((item, answer_item), set()) | {evaluation.user}\n",
    "        result[-1].total_evaluations.update([\"total\"])\n",
    "        result[-1].users.add(evaluation.user)\n",
    "    return result\n",
    "\n",
    "\n",
    "stats = extract_stats(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def highlight_by_question(ignore_columns):\n",
    "    color_switch = True\n",
    "    def inner(row):\n",
    "        nonlocal color_switch\n",
    "        color_switch = (not color_switch) if row[\"Question\"] else color_switch\n",
    "        base = [f\"background-color: {'#F9F9F9' if color_switch else 'white'}\"] * len(row)\n",
    "        for i in ignore_columns:\n",
    "            base[i] = \"background-color: white; border-right: 1px solid #dddddd\"\n",
    "        return base\n",
    "    return inner\n",
    "\n",
    "def make_header(ev):\n",
    "    display(Markdown(f\"## {ev.task_id} {ev.task_name}\\n### {ev.module_id} {ev.module_name}\"))\n",
    "\n",
    "    \n",
    "GREEN = \"#5fd65f\"\n",
    "RED = \"#FF9393\"\n",
    "def is_correct_answer(row):\n",
    "    return row[\"Correct\"].lower() == \"yes\"\n",
    "\n",
    "def color_by_correctness(cols, stat):\n",
    "    def inner(row):\n",
    "        styles = [\"\" for _ in range(len(row))]\n",
    "        for id_ in cols:\n",
    "            styles[id_] = \"background-color: \" + (GREEN if is_correct_answer(row) else RED)\n",
    "        return styles\n",
    "    return inner\n",
    "\n",
    "def custom_bar(col_index, vmin, vmax):\n",
    "    def inner(row):\n",
    "        base = [\"\" for _ in range(len(row))]\n",
    "        val = row.iloc[col_index]\n",
    "        ratio = f\"{(val - vmin)/vmax * 100:.1f}\"\n",
    "        base[col_index] = f\"border-left: 1px solid; width: 10em; background: linear-gradient(90deg,{GREEN if is_correct_answer(row) else RED} {ratio}%, transparent {ratio}%);\"\n",
    "        return base\n",
    "    return inner\n",
    "\n",
    "def custom_gradient(col_index, cmap, ondra=False):\n",
    "    def get_hex_color(val, cmap):\n",
    "        r, g, b, a = cmap(val)\n",
    "        return f\"rgb({r * 255}, {g * 255}, {b * 255})\"\n",
    "\n",
    "    def inner(row):\n",
    "        base = [\"\" for _ in range(len(row))]\n",
    "        val = row.iloc[col_index]\n",
    "        if not ondra:\n",
    "            ratio = val / 100\n",
    "        else:\n",
    "            ratio = 0.5 - val / 200 * (-1 if is_correct_answer(row) else 1)\n",
    "        base[col_index] = f\"background-color: {get_hex_color(ratio, cmap)}!important;\"\n",
    "        if not ondra and val > 85:\n",
    "            base[col_index] += \"color: white!important\"\n",
    "        return base\n",
    "    return inner\n",
    "\n",
    "def show_evaluations_stats(evaluation_stats):\n",
    "    module_data = {\n",
    "        id: json.loads(data)[\"quiz\"] for id, data in \n",
    "            session.query(model.Module.id, model.Module.data).filter(model.Module.type == model.ModuleType.QUIZ).all()\n",
    "    }\n",
    "    \n",
    "    for stat in evaluation_stats:\n",
    "        this_module_data = module_data[stat.evaluation.module_id]\n",
    "        data = []\n",
    "        total_evaluations = stat.total_evaluations[\"total\"]\n",
    "        total_users = len(stat.users)\n",
    "\n",
    "        for answer, count in sorted(stat.item_counts.items()):\n",
    "            item_i, user_a = answer\n",
    "            ticked_ratio = count / total_evaluations\n",
    "            ticked_ratio_user = len(stat.item_users[(item_i, user_a)]) / total_users\n",
    "            try:\n",
    "                data.append((\n",
    "                    item_i,\n",
    "                    user_a,\n",
    "                    this_module_data[item_i][\"question\"] if not data or data[-1][0] != item_i else \"\",\n",
    "                    this_module_data[item_i][\"options\"][user_a],\n",
    "                    ticked_ratio * 100,\n",
    "                    (ticked_ratio if user_a in stat.correct_answer[item_i] else 1 - ticked_ratio) * 100,\n",
    "                    \"Yes\" if user_a in stat.correct_answer[item_i] else \"No\",\n",
    "                    count,\n",
    "                    ticked_ratio_user * 100\n",
    "                ))\n",
    "            except IndexError:\n",
    "                data.append((\n",
    "                    item_i,\n",
    "                    user_a,\n",
    "                    \"ERROR: some question in this module probably removed\",\n",
    "                    \"ERROR: some question in this module probably removed\",\n",
    "                    ticked_ratio * 100,\n",
    "                    (ticked_ratio if user_a in stat.correct_answer[item_i] else 1 - ticked_ratio) * 100,\n",
    "                    \"Yes\" if user_a in stat.correct_answer[item_i] else \"No\",\n",
    "                    count,\n",
    "                    ticked_ratio_user * 100\n",
    "                ))\n",
    "        \n",
    "        df = pd.DataFrame(data, columns=[\n",
    "            \"Item ID\",\n",
    "            \"Answer ID\",\n",
    "            \"Question\",\n",
    "            \"Answer\",\n",
    "            \"Ticked in/All evals ratio\",\n",
    "            \"Correct/All evals ratio\",\n",
    "            \"Correct\",\n",
    "            \"Count evals\",\n",
    "            \"Ticked at least once/All users ratio\",\n",
    "#             \"Count users\",\n",
    "        ])\n",
    "\n",
    "        s = df.style.hide_columns([\"Item ID\", \"Answer ID\", \"Correct\", \"Ticked in/All evals ratio\"])\n",
    "        s.apply(custom_bar(col_index=7, vmin=0, vmax=total_evaluations), axis=1)\n",
    "        \n",
    "        s.format({\n",
    "            'Ticked in/All evals ratio': '{:,.1f} %'.format,\n",
    "            'Correct/All evals ratio': '{:,.1f} %'.format,\n",
    "            'Ticked at least once/All users ratio': '{:,.1f} %'.format,\n",
    "        })\n",
    "        \n",
    "        s.apply(highlight_by_question(ignore_columns=[7]), axis=1)\n",
    "        s.apply(custom_gradient(col_index=5, cmap=plt.get_cmap(\"RdYlGn\")), axis=1)\n",
    "        s.apply(custom_gradient(col_index=5, cmap=plt.get_cmap(\"RdYlGn\")), axis=1)\n",
    "        s.apply(custom_gradient(col_index=8, cmap=sns.diverging_palette(20, 127, s=78, l=77, as_cmap=True), ondra=True), axis=1)\n",
    "#         s.apply(custom_gradient(col_index=4, cmap=sns.diverging_palette(20, 120, l=75, as_cmap=True)), axis=1)\n",
    "        make_header(stat.evaluation)\n",
    "        display(s)\n",
    "\n",
    "show_evaluations_stats(stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
