{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_formats = ['svg']\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Varování**: Tato nástěnka zobrazuje odpovědi z textových a kvízových modulů dohromady. V textových odpovědích se neukazují odpovědi, které zadal pouze jeden uživatel.\n",
    "\n",
    "## Jak tabulky interpretovat\n",
    "\n",
    "Každá potenciální odpověď má jeden řádek.\n",
    "\n",
    "### Correct/All evals ratio\n",
    "\n",
    "- by ideálně mělo být 100 %\n",
    "- indikuje to, v kolika procentech odevzání byla daná možnost správně (ne)zaškrtnutá\n",
    "    - i.e. 100 % by znamenalo, že pokud je to\n",
    "        - správná odpověď, tak byla ve všech odevzdáních zaškrtnuta\n",
    "        - špatná odpověď, tak nebyla v žádném odevzdání zaškrtnuta\n",
    "- potenciální problém: jeden řešitel, který neví a tipuje, má větší efekt na procento než řešitel, který to dá na první pokus správně\n",
    "\n",
    "**Důsledky pro seminář**:\n",
    "\n",
    "- odpovědi, které mají nízké procento (třeba méně než < 80 %), jsou \"chytáky\", nebo potenciálně špatně vysvětlené\n",
    "\n",
    "\n",
    "### Count evals\n",
    "\n",
    "- absolutní počet odevzdání, které měli danou odpověď zaškrtnutou\n",
    "- pro správné odpovědi je podkreslení zelené\n",
    "- pro špatné odpovědi je podkreslení červené\n",
    "- potenciální problém stejný jako pro předchozí sloupec: jeden řešitel, který neví a tipuje, má větší efekt na procento než řešitel, který to dá na první pokus správně\n",
    "\n",
    "\n",
    "### Ticked at least once/All users ratio\n",
    "\n",
    "- Procento řešitelů, kteří odpověď zaškrtli alespoň jednou, ze všech řešitelů, kteří modul řešili.\n",
    "- Pro špatné odpovědi bychom ideálně chtěli 0 %, pro správně odpovědi ideálně 100 %.\n",
    "- 100 % na správné odpovědi je špatně dosažitelných, stačí jeden řešitel jež úlohu odevzdal neúspěšně a pak ji nikdy úspěšně nedokončil.\n",
    "\n",
    "**Důsledky pro seminář**:\n",
    "\n",
    "Špatné odpovědi by měly mít malé procento, pokud nemají, tak jsou to chytáky, nebo špatně vysvětlené.\n",
    "\n",
    "Správné odpovědi by měli mít velké procento, pokud nemají, tak jsme nejspíš nedostatečně vysvětlili, proč je tvrzení pravdivé.\n",
    "\n",
    "Opakovaně neúspěšní/tipující řešitelé nemají takový vliv na výsledná procenta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict, namedtuple\n",
    "from sqlalchemy import func, distinct, text, and_\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import util\n",
    "from util.year import year as current_year\n",
    "from db import session\n",
    "import model\n",
    "from datetime import datetime\n",
    "import re\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "plt.rcParams['figure.figsize'] = [8, 6]\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations = session.query(\n",
    "    model.EvaluationParticipantsWithContext\n",
    ")\\\n",
    ".filter(model.EvaluationParticipantsWithContext.year_id == current_year.id)\\\n",
    ".filter(model.EvaluationParticipantsWithContext.type.in_([model.ModuleType.QUIZ, model.ModuleType.TEXT]))\\\n",
    ".order_by(model.EvaluationParticipantsWithContext.task_id, model.EvaluationParticipantsWithContext.module_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = evaluations.all()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "TextAnswer = namedtuple(\"TextAnswer\", [\"user_answers\", \"is_correct\"])\n",
    "QuizAnswer = namedtuple(\"QuizAnswer\", [\"user_answers\", \"correct_answers\"])\n",
    "Answer = namedtuple(\"Answer\", [\"type\", \"evaluation\", \"answer\"])\n",
    "\n",
    "TextStats = namedtuple(\"TextStats\", [\"correct_answers\", \"answer_counts\", \"answer_users\", \"total_evaluations\", \"users\"])\n",
    "QuizStats = namedtuple(\"QuizStats\", [\"correct_answer\", \"combination_counts\", \"item_counts\", \"item_users\", \"total_evaluations\", \"users\"])\n",
    "Stats = namedtuple(\"Stats\", [\"type\", \"evaluation\", \"stats\"])\n",
    "\n",
    "RE_TEXT_ANSWER_LINE = re.compile(r'Raw data: \\[(.*)\\]')\n",
    "RE_QUIZ_ANSWER_LINE = re.compile(r\"^\\s*\\[[yn]\\] Question \\d+ -- user answers: \\[(.*)\\], correct answers: \\[(.*)\\]\")\n",
    "\n",
    "\n",
    "def quiz_answer_to_tuple(answer):\n",
    "    return tuple() if not answer else tuple(int(a) for a in answer.split(\", \"))\n",
    "\n",
    "\n",
    "def text_answer_to_tuple(answer):\n",
    "    assert answer\n",
    "    parts = answer.split(\", \")\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(parts):\n",
    "        if parts[i].count('\"') - parts[i].count(r'\\\"') + parts[i].count(r'\\\\\"') == 1:\n",
    "            assert i + 1 < len(parts)\n",
    "            parts[i] += \", \" + parts[i+1]\n",
    "            parts.pop(i+1)\n",
    "        else:\n",
    "            i += 1\n",
    "    \n",
    "    assert all(p[0] == p[-1] == '\"' for p in parts)\n",
    "    return tuple([p[1:-1].strip() for p in parts])\n",
    "\n",
    "\n",
    "def extract_text_answer(evaluation):\n",
    "    lines = evaluation.full_report.split(\"\\n\")\n",
    "    for line in lines:\n",
    "        match = RE_TEXT_ANSWER_LINE.match(line)\n",
    "        if match:\n",
    "            return TextAnswer(text_answer_to_tuple(match.group(1)), evaluation.ok)\n",
    "    assert False, f\"unreachable for evaluation {evaluation.full_report}\"\n",
    "    \n",
    "\n",
    "def extract_quiz_answer(evaluation):\n",
    "    lines = evaluation.full_report.split(\"\\n\")\n",
    "    user_answers = []\n",
    "    correct_answers = []\n",
    "    for line in lines:\n",
    "        match = RE_QUIZ_ANSWER_LINE.match(line)\n",
    "        if match:\n",
    "            user_answers.append(quiz_answer_to_tuple(match.group(1)))\n",
    "            correct_answers.append(quiz_answer_to_tuple(match.group(2)))\n",
    "    return QuizAnswer(tuple(user_answers), tuple(correct_answers))\n",
    "\n",
    "\n",
    "def extract_answer(evaluation):\n",
    "    if evaluation.type == model.ModuleType.TEXT:\n",
    "        answer = extract_text_answer(evaluation)\n",
    "    elif evaluation.type == model.ModuleType.QUIZ:\n",
    "        answer = extract_quiz_answer(evaluation)\n",
    "    else:\n",
    "        assert False, f\"unreachable for type {evaluation.type}\"\n",
    "    return Answer(evaluation.type, evaluation, answer)\n",
    "\n",
    "\n",
    "def partition_on_module_id(answers):\n",
    "    result = [[answers[0]]]\n",
    "    for i in range(1, len(answers)):\n",
    "        prev_module_id = result[-1][-1].evaluation.module_id\n",
    "        this_module_id = answers[i].evaluation.module_id\n",
    "        if prev_module_id == this_module_id:\n",
    "            result[-1].append(answers[i])\n",
    "        else:\n",
    "            result.append([answers[i]])\n",
    "    return result\n",
    "\n",
    "\n",
    "def extract_text_stats(answers):\n",
    "    total_subanswers = len(answers[0].answer.user_answers)\n",
    "    stats = TextStats(\n",
    "        correct_answers = [set() for _ in range(total_subanswers)],\n",
    "        answer_counts = [Counter() for _ in range(total_subanswers)],\n",
    "        answer_users = [{} for _ in range(total_subanswers)],\n",
    "        total_evaluations = {\"total\": len(answers)},\n",
    "        users = set(a.evaluation.user for a in answers),\n",
    "    )\n",
    "\n",
    "    for answer in answers:\n",
    "        user_answer = answer.answer.user_answers\n",
    "        for i, subanswer in enumerate(user_answer):\n",
    "            if answer.answer.is_correct:\n",
    "                stats.correct_answers[i].add(subanswer)\n",
    "            stats.answer_counts[i].update([subanswer])\n",
    "            stats.answer_users[i][subanswer] = stats.answer_users[i].get(subanswer, set()) | {answer.evaluation.user}\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "def extract_quiz_stats(answers):\n",
    "    stats = QuizStats(\n",
    "        correct_answer = answers[0].answer.correct_answers,\n",
    "        combination_counts = Counter(),\n",
    "        item_counts = Counter(),\n",
    "        item_users = {},\n",
    "        total_evaluations = {\"total\": len(answers)},\n",
    "        users = set(a.evaluation.user for a in answers),\n",
    "    )\n",
    "\n",
    "    for answer in answers:\n",
    "        user_answers = answer.answer.user_answers\n",
    "        stats.combination_counts.update([user_answers])\n",
    "        stats.item_counts.update([(item, answer_item) for item, answer in enumerate(user_answers) for answer_item in answer])\n",
    "        for item, ans in enumerate(user_answers):\n",
    "            for answer_item in ans:\n",
    "                stats.item_users[(item, answer_item)] = stats.item_users.get((item, answer_item), set()) | {answer.evaluation.user}\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "def extract_stats(evaluations):\n",
    "    if not evaluations:\n",
    "        return []\n",
    "\n",
    "    answers = [extract_answer(e) for e in evaluations]\n",
    "    answers_per_module = partition_on_module_id(answers)\n",
    "\n",
    "    result = []\n",
    "    for module_answers in answers_per_module:\n",
    "        evaluation = module_answers[0].evaluation\n",
    "        if evaluation.type == model.ModuleType.TEXT:\n",
    "            stats = extract_text_stats(module_answers)\n",
    "        elif evaluation.type == model.ModuleType.QUIZ:\n",
    "            stats = extract_quiz_stats(module_answers)\n",
    "        else:\n",
    "            assert False, f\"unreachable for type {evaluation.type}\"\n",
    "        result.append(Stats(evaluation.type, evaluation, stats))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "stats = extract_stats(out)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def highlight_by_question(ignore_columns):\n",
    "    color_switch = True\n",
    "    def inner(row):\n",
    "        nonlocal color_switch\n",
    "        color_switch = (not color_switch) if row[\"Question\"] else color_switch\n",
    "        base = [f\"background-color: {'#F9F9F9' if color_switch else 'white'}\"] * len(row)\n",
    "        for i in ignore_columns:\n",
    "            base[i] = \"background-color: white; border-right: 1px solid #dddddd\"\n",
    "        return base\n",
    "    return inner\n",
    "\n",
    "\n",
    "def make_header(ev):\n",
    "    display(Markdown(f\"## {ev.task_id} {ev.task_name}\\n### {ev.module_id} {ev.module_name}\"))\n",
    "\n",
    "    \n",
    "GREEN = \"#5fd65f\"\n",
    "RED = \"#FF9393\"\n",
    "\n",
    "\n",
    "def is_correct_answer(row):\n",
    "    return row[\"Correct\"].lower() == \"yes\"\n",
    "\n",
    "\n",
    "def color_by_correctness(cols, stat):\n",
    "    def inner(row):\n",
    "        styles = [\"\" for _ in range(len(row))]\n",
    "        for id_ in cols:\n",
    "            styles[id_] = \"background-color: \" + (GREEN if is_correct_answer(row) else RED)\n",
    "        return styles\n",
    "    return inner\n",
    "\n",
    "\n",
    "def custom_bar(col_index, vmin, vmax):\n",
    "    def inner(row):\n",
    "        base = [\"\" for _ in range(len(row))]\n",
    "        val = row.iloc[col_index]\n",
    "        ratio = f\"{(val - vmin)/vmax * 100:.1f}\"\n",
    "        base[col_index] = f\"border-left: 1px solid; width: 10em; background: linear-gradient(90deg,{GREEN if is_correct_answer(row) else RED} {ratio}%, transparent {ratio}%);\"\n",
    "        return base\n",
    "    return inner\n",
    "\n",
    "\n",
    "def custom_gradient(col_index, cmap, ondra=False):\n",
    "    def get_hex_color(val, cmap):\n",
    "        r, g, b, a = cmap(val)\n",
    "        return f\"rgb({r * 255}, {g * 255}, {b * 255})\"\n",
    "\n",
    "    def inner(row):\n",
    "        base = [\"\" for _ in range(len(row))]\n",
    "        val = row.iloc[col_index]\n",
    "        if not ondra:\n",
    "            ratio = val / 100\n",
    "        else:\n",
    "            ratio = 0.5 - val / 200 * (-1 if is_correct_answer(row) else 1)\n",
    "        base[col_index] = f\"background-color: {get_hex_color(ratio, cmap)}!important;\"\n",
    "        if not ondra and val > 85:\n",
    "            base[col_index] += \"color: white!important\"\n",
    "        return base\n",
    "    return inner\n",
    "\n",
    "\n",
    "def show_quiz_stats(this_module_data, stats):\n",
    "    data = []\n",
    "    total_evaluations = stats.total_evaluations[\"total\"]\n",
    "    total_users = len(stats.users)\n",
    "\n",
    "    for answer, count in sorted(stats.item_counts.items()):\n",
    "        item_i, user_a = answer\n",
    "        ticked_ratio = count / total_evaluations\n",
    "        ticked_ratio_user = len(stats.item_users[(item_i, user_a)]) / total_users\n",
    "        try:\n",
    "            data.append((\n",
    "                item_i,\n",
    "                user_a,\n",
    "                this_module_data[item_i][\"question\"] if not data or data[-1][0] != item_i else \"\",\n",
    "                this_module_data[item_i][\"options\"][user_a],\n",
    "                ticked_ratio * 100,\n",
    "                (ticked_ratio if user_a in stats.correct_answer[item_i] else 1 - ticked_ratio) * 100,\n",
    "                \"Yes\" if user_a in stats.correct_answer[item_i] else \"No\",\n",
    "                count,\n",
    "                ticked_ratio_user * 100\n",
    "            ))\n",
    "        except IndexError:\n",
    "            data.append((\n",
    "                item_i,\n",
    "                user_a,\n",
    "                \"ERROR: some question in this module probably removed\",\n",
    "                \"ERROR: some question in this module probably removed\",\n",
    "                ticked_ratio * 100,\n",
    "                (ticked_ratio if user_a in stats.correct_answer[item_i] else 1 - ticked_ratio) * 100,\n",
    "                \"Yes\" if user_a in stats.correct_answer[item_i] else \"No\",\n",
    "                count,\n",
    "                ticked_ratio_user * 100\n",
    "            ))\n",
    "\n",
    "    df = pd.DataFrame(data, columns=[\n",
    "        \"Item ID\",\n",
    "        \"Answer ID\",\n",
    "        \"Question\",\n",
    "        \"Answer\",\n",
    "        \"Ticked in/All evals ratio\",\n",
    "        \"Correct/All evals ratio\",\n",
    "        \"Correct\",\n",
    "        \"Count evals\",\n",
    "        \"Ticked at least once/All users ratio\",\n",
    "    ])\n",
    "\n",
    "    s = df.style.hide_columns([\"Item ID\", \"Answer ID\", \"Correct\", \"Ticked in/All evals ratio\"])\n",
    "    s.apply(custom_bar(col_index=7, vmin=0, vmax=total_evaluations), axis=1)\n",
    "\n",
    "    s.format({\n",
    "        'Ticked in/All evals ratio': '{:,.1f} %'.format,\n",
    "        'Correct/All evals ratio': '{:,.1f} %'.format,\n",
    "        'Ticked at least once/All users ratio': '{:,.1f} %'.format,\n",
    "    })\n",
    "\n",
    "    s.apply(highlight_by_question(ignore_columns=[7]), axis=1)\n",
    "    s.apply(custom_gradient(col_index=5, cmap=plt.get_cmap(\"RdYlGn\")), axis=1)\n",
    "    s.apply(custom_gradient(col_index=8, cmap=sns.diverging_palette(20, 127, s=78, l=77, as_cmap=True), ondra=True), axis=1)\n",
    "    display(s)\n",
    "\n",
    "    \n",
    "def show_text_stats(this_module_data, stats):\n",
    "    data = []\n",
    "    total_evaluations = stats.total_evaluations[\"total\"]\n",
    "    total_users = len(stats.users)\n",
    "    \n",
    "    if \"diff\" in this_module_data:\n",
    "        display(Markdown(f\"Evaluated by diff with `{this_module_data['diff']}`.\"))\n",
    "    else:\n",
    "        display(Markdown(f\"Evaluated by the eval script at `{this_module_data['eval_script']}`.\"))\n",
    "\n",
    "    for i in range(len(stats.answer_counts)):\n",
    "        for j, (answer, count) in enumerate(sorted(stats.answer_counts[i].items(), key=lambda p: (p[1], p[0]), reverse=True)):\n",
    "            submitted_in_evals_ratio = count / total_evaluations\n",
    "            submitted_by_users = len(stats.answer_users[i][answer])\n",
    "            submitted_by_users_ratio = submitted_by_users / total_users\n",
    "            if submitted_by_users > 1:\n",
    "                data.append((\n",
    "                    this_module_data[\"questions\"][i].strip() if j == 0 else \"\",\n",
    "                    f\"<code>{answer}</code>\",\n",
    "                    submitted_in_evals_ratio * 100,\n",
    "                    (submitted_in_evals_ratio if answer in stats.correct_answers[i] else 1 - submitted_in_evals_ratio) * 100,\n",
    "                    \"Yes\" if answer in stats.correct_answers[i] else \"No\",\n",
    "                    count,\n",
    "                    submitted_by_users_ratio * 100\n",
    "                ))\n",
    "\n",
    "    df = pd.DataFrame(data, columns=[\n",
    "        \"Question\",\n",
    "        \"Answer\",\n",
    "        \"Submitted in/All evals ratio\",\n",
    "        \"Correct/All evals ratio\",\n",
    "        \"Correct\",\n",
    "        \"Count evals\",\n",
    "        \"Submitted at least once/All users ratio\",\n",
    "    ])\n",
    "\n",
    "    s = df.style.hide_columns([\"Correct/All evals ratio\"])\n",
    "    s.apply(custom_bar(col_index=5, vmin=0, vmax=total_evaluations), axis=1)\n",
    "\n",
    "    s.format({\n",
    "        'Submitted in/All evals ratio': '{:,.1f} %'.format,\n",
    "        'Correct/All evals ratio': '{:,.1f} %'.format,\n",
    "        'Submitted at least once/All users ratio': '{:,.1f} %'.format,\n",
    "    })\n",
    "\n",
    "    s.apply(custom_gradient(col_index=3, cmap=plt.get_cmap(\"RdYlGn\")), axis=1)\n",
    "    s.apply(custom_gradient(col_index=6, cmap=sns.diverging_palette(20, 127, s=78, l=77, as_cmap=True), ondra=True), axis=1)\n",
    "    s.set_table_styles([\n",
    "        {\"selector\": \".col1 code\", \"props\": [(\"background-color\", \"#def2fc\")]}\n",
    "    ])\n",
    "\n",
    "    display(s)\n",
    "\n",
    "\n",
    "def show_evaluations_stats(evaluation_stats):\n",
    "    module_data = {\n",
    "        id: json.loads(data) for id, data in \n",
    "            session.query(model.Module.id, model.Module.data)\\\n",
    "            .filter(model.Module.type.in_([model.ModuleType.QUIZ, model.ModuleType.TEXT])).all()\n",
    "    }\n",
    "    \n",
    "    for stats in evaluation_stats:\n",
    "        make_header(stats.evaluation)\n",
    "        if stats.type == model.ModuleType.TEXT:\n",
    "            show_text_stats(module_data[stats.evaluation.module_id][\"text\"], stats.stats)\n",
    "        elif stats.type == model.ModuleType.QUIZ:\n",
    "            show_quiz_stats(module_data[stats.evaluation.module_id][\"quiz\"], stats.stats)\n",
    "            pass\n",
    "        else:\n",
    "            assert False, f\"unreachable for type {stats.type}\"\n",
    "\n",
    "\n",
    "show_evaluations_stats(stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
