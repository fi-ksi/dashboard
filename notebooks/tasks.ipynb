{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KSI monitoring: úlohy\n",
    "\n",
    "V tomto souboru se nachází hlavní tabulka statistik týkajících se úloh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_formats = ['svg']\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict, namedtuple\n",
    "from sqlalchemy import func, distinct, text, and_\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import util\n",
    "from util.year import year as current_year\n",
    "from db import session\n",
    "import model\n",
    "from datetime import datetime\n",
    "\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "plt.rcParams['figure.figsize'] = [8, 6]\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations = session.query(\n",
    "    model.Task,\n",
    "    func.count(distinct(model.User.id)).label('evals_count'),\n",
    ").\\\n",
    "    join(model.Task.r_wave).filter(model.Wave.year == current_year.id).\\\n",
    "    join(model.Task.modules).\\\n",
    "    join(model.Module.evaluations).\\\n",
    "    join(model.Evaluation.r_user).\\\n",
    "    filter(model.User.role == 'participant')\n",
    "\n",
    "evaluations_per_task = evaluations.\\\n",
    "    group_by(model.Task.id).order_by(model.Wave.id, model.Task.id)\n",
    "\n",
    "evaluations_per_task_d = {\n",
    "    task: evals_count\n",
    "    for (task, evals_count) in evaluations_per_task.all()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "successful_evaluations = evaluations_per_task.\\\n",
    "    filter(model.Evaluation.ok == True)\n",
    "\n",
    "successful_evaluations_d = {\n",
    "    task: evals_count\n",
    "    for (task, evals_count) in successful_evaluations.all()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "per_module = session.query(\n",
    "    model.Evaluation.user.label('user_id'),\n",
    "    func.count(model.Evaluation.id).label('eval_count'),\n",
    ").\\\n",
    "    join(model.Evaluation.r_module).join(model.Module.r_task).\\\n",
    "    join(model.Task.r_wave).\\\n",
    "    filter(model.Wave.year == current_year.id).\\\n",
    "    group_by(model.Evaluation.user,\n",
    "             model.Evaluation.module).subquery()\n",
    "\n",
    "EVAL_LIMITS = [10, 30, 50]\n",
    "problematic_tasks = {\n",
    "    limit: {\n",
    "        task: evals_count\n",
    "        for (task, evals_count) in (\n",
    "            evaluations.\n",
    "            filter(model.Evaluation.ok == False).\n",
    "            join(per_module, per_module.c.user_id == model.User.id).\\\n",
    "            group_by(model.Task.id).\\\n",
    "            filter(per_module.c.eval_count > limit).all()\n",
    "        )\n",
    "    }\n",
    "    for limit in EVAL_LIMITS\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_count = session.query(\n",
    "    model.Task,\n",
    "    func.count(model.Post.id)\n",
    ").\\\n",
    "    join(model.Task.r_wave).\\\n",
    "    filter(model.Wave.year == current_year.id).\\\n",
    "    join(model.Task.discussion_posts).\\\n",
    "    group_by(model.Task.id).\\\n",
    "    all()\n",
    "\n",
    "posts_count_dict = {\n",
    "    task: count\n",
    "    for (task, count) in posts_count\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_waves = session.query(model.Wave).\\\n",
    "    filter(model.Wave.year == current_year.id).\\\n",
    "    order_by(model.Wave.index).all()\n",
    "\n",
    "large_tasks = util.task.large_tasks().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_evaluations_per_task_and_user = session.query(\n",
    "    model.Task,\n",
    "    model.User,\n",
    "    func.count(model.Evaluation.id),\n",
    ").\\\n",
    "    join(model.Task.r_wave).filter(model.Wave.year == current_year.id).\\\n",
    "    join(model.Task.modules).\\\n",
    "    join(model.Module.evaluations).\\\n",
    "    join(model.Evaluation.r_user).\\\n",
    "    filter(model.User.role == 'participant').\\\n",
    "    group_by(model.Task.id, model.User.id).\\\n",
    "    all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_evaluations_dict = {}\n",
    "for task, user, count in no_evaluations_per_task_and_user:\n",
    "    if task in no_evaluations_dict:\n",
    "        no_evaluations_dict[task].append(count)\n",
    "    else:    \n",
    "        no_evaluations_dict[task] = [count]\n",
    "        \n",
    "no_evaluations_avg = {\n",
    "    task: np.average(counts)\n",
    "    for task, counts in no_evaluations_dict.items()\n",
    "}\n",
    "\n",
    "no_evaluations_median = {\n",
    "    task: int(np.median(counts))\n",
    "    for task, counts in no_evaluations_dict.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def show_wave_stats(wave, max_evals_count):\n",
    "    tasks = []\n",
    "    for task in evaluations_per_task_d:\n",
    "        try:\n",
    "            if task.wave != wave.id:\n",
    "                continue\n",
    "            tasks.append(\n",
    "                (\n",
    "                    task,\n",
    "                    evaluations_per_task_d[task],\n",
    "                    successful_evaluations_d[task],\n",
    "                    successful_evaluations_d[task] / evaluations_per_task_d[task],\n",
    "                    evaluations_per_task_d[task]-successful_evaluations_d[task],\n",
    "                    problematic_tasks[10][task] if task in problematic_tasks[10] else 0,\n",
    "                    problematic_tasks[30][task] if task in problematic_tasks[30] else 0,\n",
    "                    problematic_tasks[50][task] if task in problematic_tasks[50] else 0,\n",
    "                    posts_count_dict[task] if task in posts_count_dict else 0,\n",
    "                    no_evaluations_avg[task] if task in no_evaluations_avg else '-',\n",
    "                    no_evaluations_median[task] if task in no_evaluations_median else '-',\n",
    "                    sorted(no_evaluations_dict[task])[-5:] if task in no_evaluations_dict else '-',\n",
    "                )\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\"Failed for task\", task, e)\n",
    "    tasks.sort(key=lambda x: (x[0] in large_tasks, x[0].id))\n",
    "\n",
    "    df = pd.DataFrame(tasks, columns=[\n",
    "        'Task',\n",
    "        'All Evaluations',\n",
    "        'Successful Evaluations',\n",
    "        'Successful/All ratio',\n",
    "        'N.O. Users failing on task now',\n",
    "        'N.O. Users with more than 10 unsucc. evaluations',\n",
    "        'N.O. Users with more than 30 unsucc. evaluations',\n",
    "        'N.O. Users with more than 50 unsucc. evaluations',\n",
    "        'Discussion Posts Count',\n",
    "        'Average N.O. Evaluations Per User',\n",
    "        'Median N.O. Evaluations Per User',\n",
    "        'Some Maximum Evaluations Counts',\n",
    "    ]).set_index('Task')\n",
    "    \n",
    "    s = df.style\n",
    "    \n",
    "    s.background_gradient(subset=[\n",
    "        'N.O. Users failing on task now',\n",
    "        'N.O. Users with more than 10 unsucc. evaluations',\n",
    "        'N.O. Users with more than 30 unsucc. evaluations',\n",
    "        'N.O. Users with more than 50 unsucc. evaluations',\n",
    "        'Average N.O. Evaluations Per User',\n",
    "        'Median N.O. Evaluations Per User',        \n",
    "    ], cmap=sns.light_palette(\"red\", as_cmap=True))\n",
    "    \n",
    "    s.background_gradient(\n",
    "        subset=['Successful/All ratio'],\n",
    "        cmap=sns.light_palette(\"green\", as_cmap=True)\n",
    "    )\n",
    "    \n",
    "    s.background_gradient(\n",
    "        subset=['Discussion Posts Count'],\n",
    "        cmap=sns.light_palette(\"orange\", as_cmap=True)\n",
    "    )\n",
    "    \n",
    "    s.bar(subset=['All Evaluations'], color='#5fd65f', vmin=0, vmax=max_evals_count)\n",
    "    \n",
    "    s.format({\n",
    "        'Successful/All ratio': '{:,.1%}'.format,\n",
    "        'Average N.O. Evaluations Per User': '{:.2f}'.format,\n",
    "    })\n",
    "    \n",
    "    display(Markdown('## {name}'.format(name=wave.caption)))\n",
    "    display(s)\n",
    "\n",
    "max_evals_count = max(evaluations_per_task_d.values(), default=10)\n",
    "    \n",
    "for wave in all_waves:\n",
    "    show_wave_stats(wave, max_evals_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistika velkých úloh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "per_module = session.query(\n",
    "    model.Wave,\n",
    "    model.Task,\n",
    "    model.Evaluation.user.label('user_id'),\n",
    "    model.Evaluation.module.label('module_id'),\n",
    "    func.max(model.Evaluation.points).label('points'),\n",
    "    model.Module.max_points.label('max_points'),\n",
    ").\\\n",
    "    join(model.Evaluation.r_module).\\\n",
    "    filter(model.Module.type == model.ModuleType.GENERAL).\\\n",
    "    join(model.Module.r_task).join(model.Task.r_wave).\\\n",
    "    filter(model.Wave.year == current_year.id).\\\n",
    "    group_by(model.Task.id, model.Evaluation.user, model.Evaluation.id).\\\n",
    "    order_by(model.Wave.id)\n",
    "\n",
    "norm_points = [\n",
    "    pm.points / pm.max_points\n",
    "    for pm in per_module\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(per_module.all())\n",
    "if not df.empty:\n",
    "    df['norm_points'] = norm_points\n",
    "    grouped = df.groupby('Task')\n",
    "\n",
    "    per_task = pd.DataFrame(OrderedDict((\n",
    "        ('Wave', grouped.Wave.first()),\n",
    "        ('Solved count', grouped.user_id.count()),\n",
    "        ('Max Points', grouped.max_points.first()),\n",
    "        ('Points Average', grouped.points.mean()),\n",
    "        ('Points Median', grouped.points.median()),    \n",
    "        ('Points Average Normalized', grouped.norm_points.mean()),\n",
    "        ('Points Median Normalized', grouped.norm_points.median()),\n",
    "    )))\n",
    "    s = per_task.style\n",
    "    s.format({\n",
    "        'Max Points': '{:.1f}'.format,\n",
    "        'Points Median': '{:.2f}'.format,\n",
    "        'Points Average': '{:.2f}'.format,\n",
    "        'Points Average Normalized': '{:,.1%}'.format,\n",
    "        'Points Median Normalized': '{:,.1%}'.format,\n",
    "    })\n",
    "\n",
    "    s.background_gradient(subset=[\n",
    "        'Solved count',\n",
    "        'Points Average Normalized',\n",
    "        'Points Median Normalized',\n",
    "    ], cmap=sns.light_palette(\"green\", as_cmap=True))\n",
    "\n",
    "    s.background_gradient(subset=[\n",
    "        'Points Median',\n",
    "        'Points Average',\n",
    "    ], cmap=sns.light_palette(\"blue\", as_cmap=True))\n",
    "\n",
    "    display(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kolik řešitelů získalo kolik bodů za velké úlohy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    gained_points = pd.DataFrame(OrderedDict((\n",
    "        ('[9, inf)', df[df['points'] >= 9].groupby('Task').points.count()),    \n",
    "        ('[8–9)', df[(df['points'] >= 8) & (df['points'] < 9)].groupby('Task').points.count()),    \n",
    "        ('[6–8)', df[(df['points'] >= 6) & (df['points'] < 8)].groupby('Task').points.count()),\n",
    "        ('[4–6)', df[(df['points'] >= 4) & (df['points'] < 6)].groupby('Task').points.count()),\n",
    "\n",
    "        ('[0–4)', df[(df['points'] >= 0) & (df['points'] < 4)].groupby('Task').points.count()),\n",
    "    )))\n",
    "    gained_points.plot.bar(stacked=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Předchozí tabulka normalizovaná maximálním počtem bodů za úlohu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    gained_points_norm = pd.DataFrame(OrderedDict((\n",
    "        ('>90 %', df[df['norm_points'] >= .9].groupby('Task').norm_points.count()),    \n",
    "        ('[80 % – 90 %)', df[(df['norm_points'] >= .8) & (df['norm_points'] < .9)].groupby('Task').norm_points.count()),\n",
    "        ('[60 % – 80 %)', df[(df['norm_points'] >= .6) & (df['norm_points'] < .8)].groupby('Task').norm_points.count()),\n",
    "        ('[40 % – 60 %)', df[(df['norm_points'] >= .4) & (df['norm_points'] < .6)].groupby('Task').norm_points.count()),\n",
    "        ('[0 % – 40 %)', df[(df['norm_points'] >= 0) & (df['norm_points'] < .4)].groupby('Task').norm_points.count()),\n",
    "    )))\n",
    "    gained_points_norm.plot.bar(stacked=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
